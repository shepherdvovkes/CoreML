# Классификация запросов через LLM

## Обзор

Вместо использования regex-паттернов для определения типа запроса, система теперь использует OpenAI API для более точной классификации запросов пользователей.

## Преимущества

1. **Точность**: LLM лучше понимает контекст и намерения пользователя
2. **Гибкость**: Не нужно поддерживать список всех возможных фраз
3. **Различение контекстов**: Система может различать:
   - "Покажи текст дела 686/32982/23" → судебное дело (MCP Law)
   - "Покажи документ 686" → документ пользователя (RAG)

## Реализация

### Основной метод: `_classify_query_llm()`

Использует OpenAI API (gpt-3.5-turbo) для классификации запроса:

```python
classification = await self._classify_query_llm(query)
```

### Fallback метод: `_classify_query_regex()`

Если LLM недоступен или произошла ошибка, используется regex-классификация как резервный вариант.

### Кэширование

Результаты классификации кэшируются на 1 час для оптимизации производительности.

## Формат ответа

```json
{
  "use_law": true/false,
  "use_rag": true/false,
  "query_type": "legal|user_documents|document_text|list_documents|delete_documents|general",
  "has_case_number": true/false,
  "is_document_text_query": true/false,
  "is_list_query": true/false,
  "is_delete_query": true/false,
  "document_number": null или число
}
```

## Примеры классификации

### Судебное дело
- Запрос: "Покажи текст дела 686/32982/23"
- Результат: `use_law=true`, `use_rag=false`, `has_case_number=true`

### Документ пользователя
- Запрос: "Покажи документ 686"
- Результат: `use_law=false`, `use_rag=true`, `is_document_text_query=true`

### Общий вопрос
- Запрос: "Что такое договор?"
- Результат: `use_law=true`, `use_rag=true` (оба источника)

## Настройки

- **Модель**: `gpt-3.5-turbo` (быстрая и дешевая для классификации)
- **Temperature**: `0.1` (низкая для стабильности)
- **Max tokens**: `200` (короткий ответ)
- **TTL кэша**: `3600` секунд (1 час)

## Обработка ошибок

Если LLM недоступен:
1. Логируется ошибка
2. Используется regex fallback
3. Система продолжает работать

## Производительность

- Первый запрос: ~200-500ms (зависит от OpenAI API)
- Кэшированные запросы: <1ms (из кэша)
- Fallback (regex): <1ms

## Конфигурация

Классификация использует настройки OpenAI из `config.py`:
- `openai_base_url`
- `openai_api_key`
- `default_llm_provider` (должен быть OPENAI для классификации)

