"""
–°–µ—Ä–≤–∏—Å RAG –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
"""
import os
from typing import List, Dict, Any, Optional
from .document_processor import DocumentProcessor
from .document_classifier import DocumentClassifier
from .vector_store import create_vector_store, DummyVectorStore
from core.services.cache_service import CacheService
from core.resilience import resilient_rag
from loguru import logger


class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, cache_service: Optional[CacheService] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            cache_service: –°–µ—Ä–≤–∏—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –∞–ª–≥–æ—Ä–∏—Ç–º, —á—Ç–æ –∏ –≤ —Ç–µ—Å—Ç–∞—Ö: —è–≤–Ω–æ –≤–∫–ª—é—á–∞–µ–º Vision API
        self.processor = DocumentProcessor(use_vision_api=True)
        self.vector_store = create_vector_store()
        self.cache_service = cache_service
    
    def add_document(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG —Å–∏—Å—Ç–µ–º—É
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —á–∞–Ω–∫–æ–≤
        """
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if metadata is None:
            metadata = {}
        
        filename = metadata.get('filename') or os.path.basename(file_path)
        metadata['filename'] = filename
        metadata['file_path'] = file_path
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text = self.processor.process_document(file_path)
        if not text:
            logger.warning(f"Could not extract text from {file_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ Redis –¥–∞–∂–µ –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω
            self._save_document_metadata(filename, file_path, metadata, chunks_count=0, status='error', 
                                        message='Could not extract text from document')
            
            return {
                "status": "error",
                "message": "Could not extract text from document",
                "chunks_count": 0,
                "collections": [],
                "filename": filename
            }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_type_info = DocumentClassifier.detect_document_type(text, filename)
        doc_type = doc_type_info.get("type", "unknown")
        doc_confidence = doc_type_info.get("confidence", 0.0)
        
        logger.info(f"Detected document type: {doc_type} (confidence: {doc_confidence:.2f}) for {filename}")
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.processor.chunk_text(text)
        chunks_count = len(chunks)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if metadata is None:
            metadata = {}
        metadata['source'] = file_path
        metadata['document_type'] = doc_type
        metadata['document_type_confidence'] = doc_confidence
        
        metadatas = [metadata.copy() for _ in chunks]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vector_store.add_documents(chunks, metadatas)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö
        collections = []
        try:
            if hasattr(self.vector_store, 'collection_name'):
                # Qdrant
                collections.append(self.vector_store.collection_name)
            elif hasattr(self.vector_store, 'collection'):
                # ChromaDB
                if hasattr(self.vector_store.collection, 'name'):
                    collections.append(self.vector_store.collection.name)
                else:
                    collections.append("legal_documents")  # Default –¥–ª—è ChromaDB
            else:
                # DummyVectorStore –∏–ª–∏ –¥—Ä—É–≥–∏–µ
                collections.append("vector_store")
        except Exception as e:
            logger.warning(f"Could not get collection name: {e}")
            collections.append("unknown")
        
        # –ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–∞ –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (async, –Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ sync –º–µ—Ç–æ–¥–∞)
        if self.cache_service:
            import asyncio
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ event loop
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # –ï—Å–ª–∏ loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
                        asyncio.create_task(self.cache_service.delete_pattern("rag:*"))
                    else:
                        # –ï—Å–ª–∏ loop –Ω–µ –∑–∞–ø—É—â–µ–Ω, –∑–∞–ø—É—Å–∫–∞–µ–º
                        loop.run_until_complete(self.cache_service.delete_pattern("rag:*"))
                except RuntimeError:
                    # –ù–µ—Ç event loop, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                    asyncio.run(self.cache_service.delete_pattern("rag:*"))
            except Exception as e:
                logger.warning(f"Failed to invalidate cache: {e}")
        
        logger.info(f"Document {file_path} added to RAG system: {chunks_count} chunks in {collections}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ Redis
        filename = metadata.get('filename') or os.path.basename(file_path)
        self._save_document_metadata(filename, file_path, metadata, chunks_count=chunks_count, 
                                    status='success', collections=collections)
        
        return {
            "status": "success",
            "message": f"Document processed and added to RAG system ({chunks_count} chunks)",
            "chunks_count": chunks_count,
            "collections": collections,
            "file_path": file_path,
            "filename": filename
        }
    
    @resilient_rag(name="rag_search")
    async def search(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        top_k = top_k or 5
        
        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
        if self.cache_service:
            cache_key = self.cache_service._generate_key("rag:search", query, top_k=top_k)
            cached_result = await self.cache_service.get(cache_key)
            if cached_result is not None:
                logger.debug(f"RAG search cache hit for query: {query[:50]}...")
                return cached_result
        
        # –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        results = self.vector_store.search(query, top_k)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        if self.cache_service:
            cache_key = self.cache_service._generate_key("rag:search", query, top_k=top_k)
            await self.cache_service.set(cache_key, results, ttl=3600)  # 1 —á–∞—Å
        
        return results
    
    @resilient_rag(name="rag_get_context")
    async def get_context(self, query: str, top_k: int = None) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –ö–æ–Ω—Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞
        """
        top_k = top_k or 5
        
        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
        if self.cache_service:
            cache_key = self.cache_service._generate_key("rag:context", query, top_k=top_k)
            cached_context = await self.cache_service.get(cache_key)
            if cached_context is not None:
                logger.debug(f"RAG context cache hit for query: {query[:50]}...")
                return cached_context
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        results = await self.search(query, top_k)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        context_parts = []
        current_doc = None
        current_doc_type = None
        doc_chunks = []
        
        for result in results:
            text = result.get('text', '').strip()
            if not text:
                continue
                
            metadata = result.get('metadata', {})
            filename = metadata.get('filename') or metadata.get('file_path', 'Unknown')
            doc_type = metadata.get('document_type', 'unknown')
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            if current_doc != filename:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                if current_doc and doc_chunks:
                    doc_header = f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {current_doc}"
                    if current_doc_type and current_doc_type != 'unknown':
                        doc_header += f" (—Ç–∏–ø: {current_doc_type})"
                    context_parts.append(doc_header)
                    context_parts.extend(doc_chunks)
                    context_parts.append("")  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                current_doc = filename
                current_doc_type = doc_type
                doc_chunks = []
            
            doc_chunks.append(text)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
        if current_doc and doc_chunks:
            doc_header = f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {current_doc}"
            if current_doc_type and current_doc_type != 'unknown':
                doc_header += f" (—Ç–∏–ø: {current_doc_type})"
            context_parts.append(doc_header)
            context_parts.extend(doc_chunks)
        
        context = "\n\n".join(context_parts)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        if self.cache_service:
            cache_key = self.cache_service._generate_key("rag:context", query, top_k=top_k)
            await self.cache_service.set(cache_key, context, ttl=3600)  # 1 —á–∞—Å
        
        return context
    
    def _save_document_metadata(self, filename: str, file_path: str, metadata: Dict[str, Any], 
                                chunks_count: int = 0, status: str = 'success', 
                                message: str = None, collections: List[str] = None):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Redis (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Celery tasks)
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
            status: –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –æ —Å—Ç–∞—Ç—É—Å–µ
            collections: –°–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π
        """
        if not self.cache_service:
            return
        
        try:
            from datetime import datetime
            import json
            import redis
            
            doc_metadata = {
                'filename': filename,
                'file_path': file_path,
                'chunks_count': chunks_count,
                'status': status,
                'uploaded_at': datetime.utcnow().isoformat(),
                'collections': collections or [],
                **{k: v for k, v in metadata.items() if k not in ['filename', 'file_path', 'source']}
            }
            
            if message:
                doc_metadata['message'] = message
            
            cache_key = f"document:metadata:{filename}"
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π Redis –∫–ª–∏–µ–Ω—Ç –Ω–∞–ø—Ä—è–º—É—é
            try:
                # –ü–æ–ª—É—á–∞–µ–º URL Redis –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
                from config import settings
                redis_url = settings.redis_url
                
                # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
                sync_client = redis.from_url(redis_url, decode_responses=False)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∫ JSON
                value = json.dumps(doc_metadata).encode('utf-8')
                sync_client.setex(cache_key.encode('utf-8'), 2592000, value)  # 30 –¥–Ω–µ–π
                sync_client.close()
                
                logger.debug(f"Saved document metadata to Redis: {cache_key}")
            except Exception as redis_error:
                logger.warning(f"Failed to save document metadata using sync Redis client: {redis_error}")
                # Fallback: –ø—ã—Ç–∞–µ–º—Å—è —á–µ—Ä–µ–∑ async (–µ—Å–ª–∏ –µ—Å—Ç—å event loop)
                try:
                    import asyncio
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # –ï—Å–ª–∏ loop –∑–∞–ø—É—â–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É (–Ω–æ –æ–Ω–∞ –º–æ–∂–µ—Ç –Ω–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å—Å—è)
                        asyncio.create_task(self.cache_service.set(cache_key, doc_metadata, ttl=2592000))
                    else:
                        loop.run_until_complete(self.cache_service.set(cache_key, doc_metadata, ttl=2592000))
                except Exception as async_error:
                    logger.warning(f"Failed to save document metadata via async: {async_error}")
        except Exception as e:
            logger.warning(f"Failed to save document metadata to cache: {e}")
    
    async def has_documents(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        
        Returns:
            True –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã, False –∏–Ω–∞—á–µ
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            if isinstance(self.vector_store, DummyVectorStore):
                return False
            return self.vector_store.has_documents()
        except Exception as e:
            logger.warning(f"Error checking documents: {e}")
            return False
    
    async def list_documents(self) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∏ –∏–∑ Redis (–≥–¥–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö)
        
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ Redis (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - —Ç–∞–º –µ—Å—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –¥–∞–∂–µ –±–µ–∑ —á–∞–Ω–∫–æ–≤)
            documents_from_cache = []
            if self.cache_service:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    pattern = "document:metadata:*"
                    client = await self.cache_service._get_client()
                    keys = []
                    async for key in client.scan_iter(match=pattern):
                        keys.append(key)
                    
                    logger.debug(f"Found {len(keys)} document metadata keys in Redis")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    for key in keys:
                        key_str = key.decode() if isinstance(key, bytes) else key
                        metadata = await self.cache_service.get(key_str)
                        if metadata:
                            documents_from_cache.append(metadata)
                except Exception as e:
                    logger.warning(f"Error getting documents from cache: {e}")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î (–¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —á–∞–Ω–∫–∞–º–∏)
            documents_from_vector = []
            if not isinstance(self.vector_store, DummyVectorStore):
                try:
                    documents_from_vector = self.vector_store.list_documents()
                    logger.debug(f"Found {len(documents_from_vector)} documents in vector store")
                except Exception as e:
                    logger.warning(f"Error getting documents from vector store: {e}")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∏ –∏–∑ –∫—ç—à–∞
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ filename
            documents_map = {}
            
            # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫—ç—à–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - —Ç–∞–º –µ—Å—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)
            for doc in documents_from_cache:
                filename = doc.get('filename') or doc.get('file_path')
                if filename:
                    documents_map[filename] = doc.copy()
            
            # –ó–∞—Ç–µ–º –æ–±–Ω–æ–≤–ª—è–µ–º/–¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î (–¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è chunks_count)
            for doc in documents_from_vector:
                filename = doc.get('filename') or doc.get('file_path')
                if filename:
                    if filename in documents_map:
                        # –û–±–Ω–æ–≤–ª—è–µ–º chunks_count –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
                        documents_map[filename]['chunks_count'] = doc.get('chunks_count', 0)
                        # –û–±–Ω–æ–≤–ª—è–µ–º –¥—Ä—É–≥–∏–µ –ø–æ–ª—è –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                        if doc.get('uploaded_at'):
                            documents_map[filename]['uploaded_at'] = doc.get('uploaded_at')
                    else:
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –∫—ç—à–µ)
                        documents_map[filename] = doc
            
            result = list(documents_map.values())
            logger.debug(f"Total documents after merge: {len(result)}")
            return result
        except Exception as e:
            logger.error(f"Error listing documents: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    async def delete_document(self, filename: str) -> bool:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            True –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —É–¥–∞–ª–µ–Ω, False –∏–Ω–∞—á–µ
        """
        try:
            # –£–¥–∞–ª—è–µ–º –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            deleted = self.vector_store.delete_document(filename)
            
            if deleted:
                # –£–¥–∞–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ Redis
                if self.cache_service:
                    cache_key = f"document:metadata:{filename}"
                    await self.cache_service.delete(cache_key)
                
                # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à RAG
                if self.cache_service:
                    await self.cache_service.delete_pattern("rag:*")
                
                logger.info(f"Document '{filename}' deleted successfully")
            
            return deleted
        except Exception as e:
            logger.error(f"Error deleting document '{filename}': {e}")
            return False
    
    async def get_document_chunks(self, filename: str) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            chunks = self.vector_store.get_document_chunks(filename)
            return chunks
        except Exception as e:
            logger.error(f"Error getting document chunks: {e}")
            return []
    
    async def get_document_preview_image(self, filename: str) -> Optional[bytes]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø—Ä–µ–≤—å—é)
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –ë–∞–π—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (PNG) –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            file_path = None
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
            if self.cache_service:
                cache_key = f"document:metadata:{filename}"
                metadata = await self.cache_service.get(cache_key)
                if metadata and isinstance(metadata, dict):
                    file_path = metadata.get('file_path')
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –∫—ç—à–µ, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Å–ø–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if not file_path:
                documents = await self.list_documents()
                for doc in documents:
                    if doc.get('filename') == filename or doc.get('file_path') == filename:
                        file_path = doc.get('file_path')
                        break
            
            if not file_path or not os.path.exists(file_path):
                logger.warning(f"File not found for document '{filename}': {file_path}")
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª PDF
            if not file_path.lower().endswith('.pdf'):
                logger.debug(f"Document '{filename}' is not a PDF, cannot generate preview")
                return None
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            images = self.processor._pdf_to_images(file_path)
            if images and len(images) > 0:
                return images[0]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            
            return None
        except Exception as e:
            logger.error(f"Error getting document preview image: {e}")
            return None